{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1748f1a-1574-4d31-a91e-63ed4f420651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "#database_host = \"jdbc:sqlserver://azsql-training-srv.database.windows.net:1433\"\n",
    "#database_name = \"azsql-training-db\"\n",
    "#table_name = \"Harshi.SalesLTProduct\"\n",
    "#database_user = \"pyuser\"\n",
    "#database_password = \"IDWTeam@1234\"\n",
    "#database_url = \"jdbc:sqlserver://DW Training Compute Cluster.database.windows.net:1433\" + \"azsql-training-db\" + database_name + \"pyuser\" + database_user + \"IDWTeam@1234\" + database_password + \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" + driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b895a3-2c97-4d98-85aa-ad2610181a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defined connection parameters for the JDBC connection\n",
    "driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "database_host = \"azsql-training-srv.database.windows.net\"\n",
    "database_name = \"azsql-training-db\"\n",
    "database_user = \"pyuser\"\n",
    "database_password = \"IDWteam@1234\"\n",
    "\n",
    "#  JDBC URL\n",
    "jdbc_url = f\"jdbc:sqlserver://{database_host}:1433;database={database_name}\"\n",
    "\n",
    "# Now read the data\n",
    "Table_Val = (\n",
    "    spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", jdbc_url)\n",
    "    .option(\"dbtable\", \"Harshi.SalesLTProduct\")  # Make sure this table exists\n",
    "    .option(\"user\", database_user)\n",
    "    .option(\"password\", database_password)\n",
    "    .option(\"driver\", driver)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "display(Table_Val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3fa3411-18f2-4c43-b030-0cfc7e6e141a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#product_df = spark.read.parquet(\"dbfs:/tmp/Harshi.SalesLTProduct\")\n",
    "\n",
    "#product_df = spark.read.parquet(\"dbfs:/FileStore/parquet/Harshi.SalesLTProduct\")\n",
    "\n",
    "#df1 = spark.read.option(\"header\", True).csv(\"dbfs:/databricks-datasets/online_retail/data-001/data.csv\")\n",
    "\n",
    "prodct_df = spark.read.option(\"header\", True).parquet(\"dbfs:/FileStore/product.parquet/Harshi.SalesLTProduct\")\n",
    "product_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebeb711-c09e-4201-bc3a-7f528e94db9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "product_df = spark.read.parquet(\"dbfs:/tmp/Harshi.SalesLTProduct\")\n",
    "product_df = spark.read.parquet(\"dbfs:/FileStore/product.parquet/Harshi.SalesLTProduct\")\n",
    "product_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef98b491-dfcc-4a2c-9f3c-f7fdc259f324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#product_df.write.mode(\"overwrite\").parquet(\"dbfs:/user/harshitha/product.parquet/SalesLTProduct\")\n",
    "\n",
    "product_df.write.parquet(\"dbfs:/user/harshitha/product.parquet\")\n",
    "\n",
    "#/Users/harshitha.a@idwteam.com/Harshita_space/Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e867c0-0703-40b6-a212-3eab555985f4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create a table using either Spark SQL or PySpark."
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Harshitha\", 25), (\"Vignesh\", 30), (\"Nikitha\", 27), (\"Hitaja\", 27), (\"Rajesh\", 28)]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Register as a temp table if needed\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Optionally save as a managed table in Databricks\n",
    "#df.write.mode(\"overwrite\").saveAsTable(\"people_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "132329ee-a5ca-4c31-9533-aa9970822239",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "write data from Databricks to SQL Server"
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:sqlserver://azsql-training-srv.database.windows.net:1433;databaseName=azsql-training-db\" #unable to create table directly in ssms \n",
    "table_name = \"Harshi.people_from_databricks\"\n",
    "properties = {\n",
    "    \"user\": \"pyuser\",\n",
    "    \"password\": \"IDWteam@1234\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Write to SQL Server\n",
    "df.write.jdbc(url=jdbc_url, table=table_name, mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47bcc23-7709-48ff-a555-ba93173a7e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "#created table in ssms and inserted values from data bricks \n",
    "# Sample data\n",
    "data = [(\"Harshitha\", 25), (\"Vignesh\", 26), (\"Nikitha\", 27), (\"Hitaja\", 27), (\"Rajesh\", 28)]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c35ff5d1-3f63-4c92-8b61-0844c8136495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is still named df\n",
    "\n",
    "jdbc_url = \"jdbc:sqlserver://azsql-training-srv.database.windows.net:1433;databaseName=azsql-training-db\"\n",
    "table_name = \"Harshi.People_table\"  # Replace with your actual table name\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": \"pyuser\",\n",
    "    \"password\": \"IDWteam@1234\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Append data into the existing table\n",
    "df.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=table_name,\n",
    "    mode=\"append\",  # Do NOT use 'overwrite' since the table is already there\n",
    "    properties=connection_properties\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Az_SQL_Connection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
